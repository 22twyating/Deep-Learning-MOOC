{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 單元 06-1. 使用 Model Functional API 建立各種非線性堆疊網路\n",
    "\n",
    "讓我們回顧一下生命中第一個做出來的神經網路..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 初始準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env KERAS_BACKEND=tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras functions\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# Keras dataset\n",
    "from keras.datasets import mnist\n",
    "\n",
    "# Keras utilis function\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "讀取 MNIST 手寫辨識資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape, x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np_utils.to_categorical(y_train, 10)\n",
    "y_test = np_utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Functional API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在此之前，我們使用 Sequential 便足以建構大多數的神經網路，那是因為我們接觸的神經網路多為線性堆疊 (linear stack)。\n",
    "\n",
    "除了輸入層需指定 `input_dim` 外，其餘隱藏層只需宣告，那是因為 Sequential 會認定上一層的輸出這一層的輸入。\n",
    "\n",
    "因此，再建構線性堆疊的神經網路時，Sequential 便足以處理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Functional API 的使用時機"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "當神經網路模型為非線性的複雜網路結構，如：\n",
    "\n",
    "* 多重輸出-多重輸入模型 (Multi-input and multi-output models)\n",
    "  + 分歧 (branch)\n",
    "  + 合併 (merge)\n",
    "* 具重複/循環結構的模型，如: CycleGAN\n",
    "\n",
    "Sequential 便不足以建構這類複雜結構的神經網路，我們以下介紹 `Model` Fnuctional API 的使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，我們先將 `Model` 引入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在 `Model` 的世界中，所有的神經網路層 (fully-connected, convolution, MaxPooling, LSTM, etc) 都被視作函數來操作，因此，我們只需關心函數的輸入和輸出即可。\n",
    "\n",
    "此外，為了讓神經網路的第一層從不需要輸入 `input_dim`，我們還需引進下面這個函數來代替 `input_dim`。 (此寫法亦可用在 `Sequential`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Functional API 的函數概念"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回顧一下，我們想學習的手寫辨識模型是一個長得像這樣的函數\n",
    "\n",
    "$$\\hat{f} \\colon \\mathbb{R}^{784} \\to \\mathbb{R}^{10}$$\n",
    "\n",
    "我們希望建立一個具有兩個隱藏層的神經網路來學習這個函數，攤開來看的話，如下：\n",
    "\n",
    "$$\\mathbb{R}^{784} \\overset{f_1}{\\to} \\mathbb{R}^{500} \\overset{f_2}{\\to} \\mathbb{R}^{500} \\overset{f_3}{\\to} \\mathbb{R}^{10}$$\n",
    "\n",
    "$$x \\overset{f_1}{\\mapsto} h_1 \\overset{f_2}{\\mapsto} h_2 \\overset{f_3}{\\mapsto} y$$\n",
    "\n",
    "\n",
    "或是以簡易的圖來表示這個全連接神經網路\n",
    "\n",
    "<img src=\"plain_model.png\" alt=\"drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "其中，$f_1, f_2, f_3$ 代表的是全連結層所代表的函數，其他變數說明如下：\n",
    "\n",
    "* $x$: 代表的是輸入模型的圖片向量，為 784 維的向量。\n",
    "* $h_1$: $x$ 經過第一層隱藏層運算後得結果，即為 $f_1(x)$，為 500 維的向量。\n",
    "* $h_2$: $h_1$ 經過第二層隱藏層運算後得結果，即為 $f_2(h_1)$，為 500 維的向量。\n",
    "* $y$: $h_2$ 經過最後一層運算後得結果，即為 $f_3(h_2)$，為 10 維的向量，代表的是 $x$ 為哪個數字的機率。\n",
    "\n",
    "注意: 為了方便，我們將 `Dense(500)`, `Activation('sigmoid')` 兩個合併用 `Dense(500, activation='sigmoid')` 表示"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Functional API 的操作方式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我們沿用上圖的變數名稱來定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_1 = Dense(500, activation='sigmoid')\n",
    "f_2 = Dense(500, activation='sigmoid')\n",
    "f_3 = Dense(10, activation='softmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接著，定義層前後變數之間的關係；首先，第一個變數必定以 `Input` 函數來定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Input(shape=(784,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "剩下的部分，就如變數說明，**幾乎**可以照著數學式輸入 $$h_1 = f_1(x), h_2 = f_2(h_1), y = f_3(h_2)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_1 = f_1(x)\n",
    "h_2 = f_2(h_1)\n",
    "y = f_3(h_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在這裡，變數 $h_1, h_2, y$ 是以張量 (tensor) 類別來表示，我們可以嘗試 `print` 看看。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(h_1)\n",
    "print(h_2)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接著，透過 `Model` 將一個模型的輸入/輸出包裝起來，建立模型的過程就完成了！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(x, y)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一樣的，當模型 compile 之後，便可以進行資料的訓練、預測等等，請有興趣的同學讀入 MNIST 手寫辨識之料後，自行完成這個模型的訓練。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse', optimizer=SGD(lr=0.1), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, batch_size=100, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "雖然 summary 少了很多東西，但模型架構和之前做的沒有差異，所以可以安心讀入之前訓練好的權重。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('handwriting_model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 小結論\n",
    "Functional API 的操作流程如下：\n",
    "1. 將層定義成明確的函數\n",
    "2. 透過層函數將變數連接\n",
    "3. 定義神經網路的輸入與輸出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 非線性堆疊模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 如果建立具分歧及合併結構的神經網路模型呢？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import concatenate, add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，假設我們希望在模型之間增加一個分歧，且這個分歧在模型的輸出會合併，則神經網路的結構會變成：\n",
    "\n",
    "<img src=\"branch-and-merge.png\" alt=\"drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "此模型為單一輸入、多重輸出的模型，是分歧模型最容易處理的一種。\n",
    "\n",
    "其中，$f_1, f_2$ 同之前，$f_4:\\mathbb{R}^{500}\\to\\mathbb{R}^{500}$ 的全連接層，但 `Activation` 改用 `ReLu`。\n",
    "\n",
    "需注意的是，由於 $f_3$ 的定義域改變，為 $\\mathbb{R}^{500}\\times\\mathbb{R}^{500}\\to\\mathbb{R}^{10}$ 函數，所以需要重新定義。\n",
    "\n",
    "* $x$: 代表的是輸入模型的圖片向量，為 784 維的向量。\n",
    "* $h_1$: $x$ 經過 $f_1$ 隱藏層運算後得結果，即為 $f_1(x)$，為 500 維的向量。\n",
    "* $h_2$: $h_1$ 經過 $f_2$ 隱藏層運算後得結果，即為 $f_2(h_1)$，為 500 維的向量。\n",
    "\n",
    "* $z$: $h_1$ 經過 $f_4$ 運算後得結果，即為 $f_4(h_1)$，為 500 維的向量。\n",
    "* $y$: $h_2$ 和 $z$ 經過新的 $f_3$ 運算後得結果，即為 $f_3(h_1, z)$，為 10 維的向量，代表的是 $x$ 為哪個數字的機率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因為上面已將 $f_4$ 及 $z$ 以外的變數定義好，我們只需定義 $f_3$, $f_4$ 及 $z$ 即可"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_4 = Dense(500, activation='relu')\n",
    "z = f_4(h_1)\n",
    "\n",
    "# new f_3\n",
    "f_3 = Dense(10, activation='softmax')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接著，再將 $y = f_3(h_1, z)$ 定義好，就會發現......"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 此段程式碼為錯誤範例\n",
    "y = f_3(h_2, z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "聰明的你/妳，可能會想到，函數的寫法是一次送進一個變數，那我們將 $h_3$ 和 $z$ 寫成 `list` 的形式，應該就可以送進去 $f_3$ 了吧？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 此段程式碼為錯誤範例\n",
    "y = f_3([h_2, z])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "會發現這樣也沒辦法成功。其實正確的作法是，先將 $h_2$ 與 $z$ 透過 `concatenate` 接在一起，再送進新的 $f_3$ 裡。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在這裡，我們將 $h_2$ 與 $z$ `concatenate` 接在一起，稱做 $u$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = concatenate([h_2, z])\n",
    "y = f_3(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(u)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "換句話說，模型其實是這樣畫的\n",
    "\n",
    "<img src=\"branch-and-merge_final.png\" alt=\"drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "其中，`concatenate` 是將不同的變數**接**在一起，這裡面並沒有進行任何涉及權重的運算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再透過 `Model` 將模型的輸入和輸出包裝起來，即可將模型建構完成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(x, y)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 小結論\n",
    "Branch-and-Merge 的注意要點如下：\n",
    "1. 每一層分別定義成函數\n",
    "2. 分歧結構: 實就是透過新的函數來定義新的變數，無特別注意事項。\n",
    "3. 合併結構: 要合併前，將所有要進入的變數都合併起來，才能進行之後的運算。\n",
    "\n",
    "常見應用:\n",
    "1. 多重輸入-多重輸出模型。\n",
    "2. 當層函數為 convolution 時，這樣的技巧可以實現 U-net 上的重要結構 multi-resolution fusion (多解析度融合，又稱 MRF)。\n",
    "3. ResNet 上的重要結構 skip connection (跳躍式傳遞)，亦可透過分歧-合併來實現，只是 ResNet 使用的是 `add` 而非 `concatenate`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 自定義的不具可訓練權重之神經網路層\n",
    "這裡，我們將進行這單元最後一個重要的神經網路建構技巧 - 自定義神經網路層 (不具可訓練權重)\n",
    "\n",
    "** 具有可訓練重的自定義層牽扯到 TensorFlow 及 Python 類別的撰寫，若有興趣可參考: https://keras.io/layers/writing-your-own-keras-layers/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，我們需要引入 `Lambda` 這個函數，透過 `Lambda` 函數，我們可以將 Python 上的 function，包裝成 Keras 上的 layer。\n",
    "\n",
    "此外，我們需要引進後端所使用的套件 (此處為 TensorFlow)，並使用裡面的運算進行 function 的撰寫。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.core import Lambda\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，我們透過 backend 來定義一個簡單的 function，作用是對輸入取平均，程式碼如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_function(inputs):\n",
    "    return K.mean(inputs, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接著，我們將一個 numpy array 送進這個函式，看看會發生什麼事。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 此段程式碼為錯誤範例\n",
    "average_function(np.array([1, 3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你/妳會發現，這邊的函式不接受 numpy array 的類型作為輸入，這是因為 TensorFlow 有自定義的類型，因此，在這邊我們沒辦法直接使用這個函式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我們透過 `Lambda` ，將上述函式包裝成一個神經網路層。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_layer = Lambda(average_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(average_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此時，下述程式碼可以建構出一個將指定長度的資料取平均的神經網路。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputSize = 4\n",
    "\n",
    "x = Input(shape=(inputSize,))\n",
    "y = average_layer(x)\n",
    "average_model = Model(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以發現，這樣的神經網路是不具有訓練權重的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "記得，在使用前記得先 compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_model.compile(loss='mse', optimizer='sgd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接著，我們將 `[1, 2, 3, 4]` 送進這個神經網路，看看神經網路的輸出是否為這個向量的**平均。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([[1, 2, 3, 4]]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_model.predict(np.array([[1, 2, 3, 4]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以發現，將 `[1, 2, 3, 4]` 轉成 Numpy array 送進神經網路**預測**後，答案是 `2.5`，即為 (1+2+3+4)/4，確實是平均。\n",
    "\n",
    "我們也可以一次送進多筆資料進行平均的計算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_model.predict(np.array([[1, 2, 3, 4],\n",
    "                                [1, 1, 1, 1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加碼: 具抽樣功能的神經網路層\n",
    "\n",
    "輸入為 $(\\mu, s)$ ，$\\mu=(\\mu_1,\\cdots,\\mu_n)$ 和 $s=(s_1,\\cdots,s_n)$ 各自為 $n$ 維向量。\n",
    "\n",
    "我們希望神經網路層輸出為服從 $N(\\mu, e^{s}I_n)$ 的 $n$ 維向量，換言之，我們希望建構的神經網路其實是一個抽樣函數。\n",
    "\n",
    "** 由於神經網路的輸入輸出經常沒有限制，為了讓 $s$ 具有變異數的非負特性，我們考慮 $e^{s}$ 作為變異數；換言之，$s$ 為 log-variance。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假設我們想進行抽樣的維度為 `sampling_dim`，則一個具抽樣函數功能的神經網路可由下述方式建構。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_dim = 2\n",
    "\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = K.random_normal(shape=(sampling_dim,), mean=0., stddev=1)\n",
    "    return z_mean + K.exp(z_log_var / 2) * epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這裡使用到常態分配的線性特性來定義函數，亦即\n",
    "\n",
    "$$X\\sim N(0, 1)\\Rightarrow \\mu+\\sigma X\\sim N(\\mu, \\sigma^2)$$\n",
    "\n",
    "\n",
    "若不熟機率論的同學，可以詢問助教。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_layer = Lambda(sampling, output_shape=(sampling_dim,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Input(shape=(sampling_dim,))\n",
    "s = Input(shape=(sampling_dim,))\n",
    "\n",
    "z = sample_layer([m, s])\n",
    "\n",
    "sample_model = Model([m, s], z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mean = np.random.randint(10, size=sampling_dim).reshape(1, 2)\n",
    "test_log_var = np.array([[0, 0]])\n",
    "\n",
    "print(\"平均為 (%d, %d)\" %(test_mean[0][0], test_mean[0][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "透過下面的指令，我們每次可以抽樣出一服從上述要求常態分配之隨機向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_model.predict([test_mean, test_log_var])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "來和 Numpy 上的抽樣函數進行比較吧~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_samples = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_sample = np.random.multivariate_normal(test_mean[0], np.identity(2), size=num_of_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kears_sample = np.zeros((num_of_samples, 2))\n",
    "for i in range(num_of_samples):\n",
    "    kears_sample[i] = sample_model.predict([test_mean, test_log_var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(kears_sample[:, 0], kears_sample[:, 1], 'ro')\n",
    "plt.plot(np_sample[:, 0], np_sample[:, 1], 'o')\n",
    "plt.legend(['Keras', 'Numpy'])\n",
    "plt.title('Normal Random Samples using Keras/Numpy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 恭喜你，完成學習所有建立 Variational Autoencoder 所需的重要技巧。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variational Autoencoder (VAE) 是一個重要的非監督式學習模型，具體應用的場合為特徵抽取/資料壓縮及還原，為影像處理中常見的模型之一。\n",
    "\n",
    "在建立 VAE中，需要的兩個重要技巧:\n",
    "* 分歧-合併\n",
    "* 自定義函數 (抽樣函數)\n",
    "\n",
    "已經完成了學習，下一單元，我們將告訴各位如何建立並訓練 VAE 模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
